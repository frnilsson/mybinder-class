{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro: Python and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide you with a short introduction into the basic command in Python, and specifically the basic commands of the pandas packages which will help you investigate and prepare data for subsequent analyses. If you are new to Python and need more resources you can do a free online intro class through [DataCamp](https://www.datacamp.com/courses/intro-to-python-for-data-science). \n",
    "\n",
    "After todays class you will be able to read in data from flatfiles and execute basic data manipulations using Pandas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [General Remarks](#General-Remarks)\n",
    "    1. [Python Setup](#Python-Setup)\n",
    "1. [Data Analysis in Pandas](#Data-Analysis-in-Pandas)\n",
    "    1. [Loading Data](#Load-the-Data)\n",
    "    1. [Displaying Data](#Displaying-Data)\n",
    "    1. [Columns, rows, data selection](#Columns,-rows,-data-selection)\n",
    "    1. [Subsetting Data](#Subsetting-Data)\n",
    "    1. [Statistics](#Statistics)\n",
    "    1. [Adding and Updating Data](#Adding-and-Updating-Data)\n",
    "    1. [Grouping and Aggregating Data](#Grouping-and-Aggregating-Data)\n",
    "    1. [Merging Dataframes](#Merging-Dataframes)\n",
    "    1. [Saving a CSV](#Saving-a-CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Remarks\n",
    "---\n",
    "\n",
    "Python: \n",
    "* Is a high-level interpreted general purpose programming language named after the Monty Python British comedy troupe\n",
    "* Was created by Guido van Rossum, and is maintained by an open source community\n",
    "* Is the fifth most popular programming language\n",
    "* Is an object orientied language\n",
    "* Is used mostly in data science because it is powerful and fast, and is compatible with other languages\n",
    "* Runs everywhere, it's easy to learn, it's highly readable, open-source and its fast development time compared to other languages\n",
    "* Comes with a growing and always-improving list of open-source libraries for scientific programming, data manipulation, and data analysis (e.g., Numpy, Scipy, Pandas, Scikit-Learn, Statsmodels, Matplotlib, Seaborn, PyTables, etc.)\n",
    "\n",
    "IPython/Jupyter\n",
    "* Is an enhanced, interactive python interpreter that started as a grad school project by Fernando Perez \n",
    "* Evolved into the IPython notebook, which allowed users to archive their code, figures, and analysis in a single document, making doing reproducible research and sharing said research much easier\n",
    "* Other languages including but not limited to Julia, Python and R were included. This then led to a rebranding known as the Jupyter Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. \n",
    "- NumPy is short for numerical python. NumPy is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- Pandas is a library in Python for data analysis that uses the DataFrame object from R which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack.  \n",
    "- Matplotlib is the standard plotting library in python. \n",
    "`%matplotlib inline` is a so-called \"magic\" function of Jupyter that enables plots to be displayed inline with the code and text of a notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is how the start of a notebook might look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to put this line in your notebook, otherwise the visualization won't show up\n",
    "%pylab inline\n",
    "# import the packages\n",
    "# numpy for array and matrix computation\n",
    "import numpy as np\n",
    "\n",
    "# pandas for data analysis\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib and seaborn are the data visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# configure pandas display: set the maximum number of columns displayed to 25\n",
    "pd.options.display.max_columns = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we typically load libraries like `numpy` and `pandas` with shortened aliases, e.g, `import numpy as np`. This is like saying, \"`import numpy`, and wherever you see `np`, read it as `numpy`.\" Similarly, you'll often see `import pandas as pd`, or `import matplotlib.pyplot as plt`. \n",
    "\n",
    "Another shortcut is `%pylab inline`. This command includes both `import numpy as np` and `import matplotlib.pyplot as plt `. This shortcut was invented because it's faster to type `plt.plot()` rather than `matplotlib.pyplot.plot()`, and even programmers don't like to type more than they have to. \n",
    "\n",
    "In documentation and in examples, you will frequently see `numpy` commands starting with the alias `np` rather than `numpy` (e.g, `np.array()` or `np.argsort`) and `pandas` commands starting with `pd` (e.g., `pd.DataFrame()` or `pd.concat()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how you make comments\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In object-oriented programming languages like Python, an object is an entity that contains data along with associated metadata and/or functionality. In Python everything is an object, which means every entity has some metadata (called attributes) and associated functionality (called methods). These attributes and methods are accessed via the dot syntax. Even the attributes and methods of objects are themselves objects with their own type information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python is an object based language, and these objects come with types\n",
    "x = 1         # x is an integer\n",
    "x = 'hello'   # x is a string\n",
    "x = [1, 2, 3] # x is a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to think of variables as pointers to objects, rather than of variables as buckets that contain data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And variables can point to the same objects\n",
    "x = [1, 2, 3]\n",
    "y = x\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you manipulate one the other changes as well\n",
    "x.append(4) # append 4 to the list pointed to by x\n",
    "print(y) # y's list is modified as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to do easy operations: Let's say we have a list of numbers and we want to separate them into two lists\n",
    "# 1. set the halfway point: We generated a variable and assigned the value 8 to it\n",
    "half = 4\n",
    "\n",
    "# 2. generate the two lists (; allows us to generate this in one line)\n",
    "lower = []; upper = []\n",
    "\n",
    "# 3. split the numbers into lower and upper, and assign to list\n",
    "## In Python whitespace is meaningful! block of code is a set of statements that should be treated as a unit and\n",
    "## this is indicated by the indent. Indented code blocks are always preceded by a colon (:) on the previous line\n",
    "## however: whitespace within lines does not matter\n",
    "for i in range(8):\n",
    "    if (i < half):\n",
    "        lower.append(i)\n",
    "    else:\n",
    "        upper.append(i)\n",
    "        \n",
    "print(\"lower:\", lower)\n",
    "print(\"upper:\", upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can print anything you want\n",
    "## syntax is different for python 2 and 3\n",
    "print(\"My favorite nuymbers are\", lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linebreaks (enclosing the statement in () works too)\n",
    "x = 1 + 2 + 3 + 4 + \\\n",
    "    5 + 6 + 7 + 8\n",
    "    \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are working with Pandas we are thinking in terms of dataframes. It's a pandas representation of a spreadsheet/ sql table/Stata/R or SAS dataset. It contains information such as column names, row indices (starting from 0), and the actual data. They are the basic objects on which we will perform our data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start analysing the data we have to load it into memory. We can read in different kind of data formats. The Pandas package provides many ways to load data. It allows the user to read the data from a local csv or excel file, or pull the data from a relational database. We use a function within the pandas packages that is called `pandas.read_csv` (https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check where we are first and switch into the correct directory\n",
    "%pwd\n",
    "%cd ~\n",
    "%cd \"Yandex.Disk/BigDataPubPol/data/projects\"\n",
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"FedRePORTER_PRJ_C_FY2010.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset is loaded how do we find out what is in the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The shape and the columns of the dataframe\n",
    "When we get the data, we not only want to know the column names but we also want to know how many rows and columns are in the data. We can find out the row and column numbers by calling the shape instance variable with a dot operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of a dataframe (row number, column number)\n",
    "df.shape\n",
    "# We can see how many columns and rows are in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the list of variables in data\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can print the column names into a list\n",
    "print(list(df.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can also save this list in an object in case we want to use it later\n",
    "colnames = list(df.columns.values)\n",
    "print(colnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there anything you notice here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.read_csv('FedRePORTER_PRJ_C_FY2010.csv',\n",
    "                  skipinitialspace=True,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have different file formats, such as .txt or .tsv (tab delimited) you can also use `pandas.read_csv` but you need to specify the delimiter option `delimiter='\\t'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has different types that the data is stored in, depending on what information the attribute contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas types | usage\n",
    "---|---\n",
    "object | text\n",
    "int64 | integer numbers\n",
    "float64 | floating point numbers\n",
    "bool | true false values\n",
    "datetime64 | date time values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is always good to know what type your variables are\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change our date variable to the correct type\n",
    "df['PROJECT_START_DATE'] = pd.to_datetime(df['PROJECT_START_DATE'])\n",
    "df['PROJECT_END_DATE'] = pd.to_datetime(df['PROJECT_END_DATE'])\n",
    "df['BUDGET_START_DATE'] = pd.to_datetime(df['BUDGET_START_DATE'])\n",
    "df['BUDGET_END_DATE'] = pd.to_datetime(df['BUDGET_END_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also apply that function to specific variables in your data\n",
    "# To break down long statements we encolse statement in ()\n",
    "df[['PROJECT_START_DATE', 'PROJECT_END_DATE', 'BUDGET_START_DATE', 'BUDGET_START_DATE']] = (df[['PROJECT_START_DATE', \n",
    "    'PROJECT_END_DATE', 'BUDGET_START_DATE', 'BUDGET_START_DATE']].apply(pd.to_datetime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other functions than `pandas.to_datetime` that you can use to change the types of variables such as `pandas.to_string` or `pandas.to_numeric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Are all the other variable formatted correctly? Correct the type if not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The head and tail of the dataframe\n",
    "It is also helpful to have a look at the first or last few rows of the data for a first impression, as well as a sanity check. We can call the head()/tail() methods. We can also specify how many lines we would like to see in the parentheses at the end. We choose to display 10. If not specified, by default the first 5 lines will be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last few rows of the dataframe\n",
    "# the syntax is similar to head\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort the values (by one or multiple variables)\n",
    "(df[['PROJECT_ID', 'DEPARTMENT','ORGANIZATION_NAME','PROJECT_START_DATE', \n",
    "     'FY_TOTAL_COST']].tail().sort_values(['PROJECT_START_DATE','PROJECT_ID'], ascending=[True, True]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns, rows, data selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single column selection\n",
    "If we want to select a specific column, we can use the following syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select a single column: the dataframe variable name, followed by square brackets, and then put the\n",
    "# the column name between quotes (either single or double). \n",
    "df['AGENCY'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same would be\n",
    "df.AGENCY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is more comfortable having column names in lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you want to check the values of a variable\n",
    "df.agency.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple-column selection\n",
    "to select multiple columns, wrap the column names in a python list, then put the list or tuple between the brackets after the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we selected the columns and assigned them to a new dataframe example2\n",
    "df2 = (df[['agency', 'project_title', 'fy_total_cost',\n",
    "                           'project_start_date','project_end_date']])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single/ multiple cell(s) selection\n",
    "Use the `loc` method for cell selection. Pass the row and column indices in the _square brackets_ after `loc`. Specify the row index first, and then column name, separated by a comma. Note that both indices will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single cell selection\n",
    "# select the cell in row 3 and column project_start_date\n",
    "cell = df2.loc[3, 'project_start_date']\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple cells selection\n",
    "# option 1: use a python list to explicitly list the rows/columns\n",
    "cell = df2.loc[[0, 2, 4], 'project_start_date']\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2: use colon to indicate contiguous selection\n",
    "cell = df2.loc[0:4, 'project_start_date']\n",
    "cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to select all columns from row 5, we can use a colon symbol :.\n",
    "row5 = df2.loc[5, :]\n",
    "row5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting Data\n",
    "#### Subsetting numerical data\n",
    "Similar to the `where` statement in sql, we can also select only data that meet certain condition. Depending on whether the data is numberical or string, we should choose to use different syntax for each situation. For example, if we would like to select columns that start from year 2015, we can use a larger than or equal to operator condition to subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional subsetting: put the conditional statement within the square brackets \n",
    "# the conditional statement here is that we want the cost to be higher than or equal to 50.0000.\n",
    "df3 = df2[df2['fy_total_cost'] >= 50000]\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting string/categorical data\n",
    "When the column contains string data or categorical data, the comparison operators might not be the choice for data selection. Instead, we can compare each data in a column to a target list to see if the data in column is included in the list. This is done by calling the `isin` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select specific agencies\n",
    "# we specify the target list within the parentheses of the `isin` method\n",
    "df4 = df2[df2['agency'].isin(['NIH', 'NSF'])]\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check\n",
    "df4.agency.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subsetting with multiple conditions\n",
    "If we want to subset the data with more than one condition, we can specify all the conditions and concatenate them with the python keyword `&`. Remember to put every single condition within a pair of parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both selections from above\n",
    "df5 = df2[(df2['fy_total_cost'] >= 1000000) & (df2['agency'].isin(['NIH', 'NSF']))]\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check again\n",
    "df5.agency.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "#### Descriptive stats\n",
    "Pandas has integrated some very useful tools to help us understand the distribution of the data. The `describe` method computes the most commonly used descriptive statistics, such as count, mean, standard deviation and quantiles for a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the descriptive statistics of the variables\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value counts and unique values\n",
    "For categorical values, it is often helpful to figure out what are the unique values of a given column, and the quantity of each data. Let's go back to the welfare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out how many different agencies are there in the data\n",
    "df['agency'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to count how many observations for each agency appeared in the data\n",
    "df['agency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can combine the the value counts and unique statements\n",
    "len(df['agency'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Updating Data\n",
    "#### Creating columns\n",
    "We sometimes need to creat a new column, either to save the previously calculation from other columns, or add new information to the dataframe. The syntax is given below:\n",
    "`dataframe['column_name'] = value`\n",
    "where:\n",
    "dataframe is the dataframe in which the new column is created,\n",
    "column_name is the string of the new column name, \n",
    "value is the value of the each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we can then calculate the monthly cost by dividing the project costs column by 12, \n",
    "# and assign this newly computed column to the monthly column\n",
    "df5['monthly'] = df5['fy_total_cost']/12\n",
    "df5.head().round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating Data\n",
    "#### Group by and aggregation functions\n",
    "It is possible to group the dataframe by a column, and use aggregation function on them, and sort the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate the how many grants each agency funded\n",
    "# step1: in the groupby method, we pass the column we want to group by, we can also select what columns\n",
    "# we want to carry out the operation\n",
    "# step2: use the count method to count the number of cases\n",
    "# step3: sort the value in descending order (set the ascending parameter to False)\n",
    "df_group = df.groupby('agency')['project_id'].count().sort_values(ascending=False)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other useful aggregation functions are:\n",
    "`sum()`: sum, \n",
    "`mean()`: average, \n",
    "`agg()`: use a python dictionary to specify aggregation function based on each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the aggregation function didn't return a dataframe. So we have to convert it into a dataframe if we wnat \n",
    "# to process it further\n",
    "df_group = df_group.to_frame().reset_index()\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's correct the columns names, this shouldn't be project_id but sum of all funded projects\n",
    "df_group.rename(columns={'project_id':'number of funded projects'}, inplace=True)\n",
    "df_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging Dataframes\n",
    "Pandas provides an ability to merge (join) two datasets together. You can store the results in a new dataframe. There are different ways of mergeing data: left, right, outer, inner (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(df5, df_group, on=[\"agency\"], how=\"inner\")\n",
    "merge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a CSV\n",
    "You can save a copy of your dataframe as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.to_csv(\"~/Yandex.Disk/example_data.csv\", encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
