{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll discuss how to formulate a research question in the machine learning framework; how to build, evaluate, compare, and select models; and how to reasonably and accurately interpret model results. You'll also get hands-on experience using the `scikit-learn` package in Python.\n",
    "\n",
    "This tutorial is based on chapter 6 of [Big Data and Social Science](https://github.com/BigDataSocialScience/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary of Terms\n",
    "\n",
    "\n",
    "There are a number of terms specific to Machine Learning that you will find repeatedly in this notebook. \n",
    "\n",
    "- **Learning**: In Machine Learning, you'll hear about \"learning a model.\" This is what you probably know as \n",
    "*fitting* or *estimating* a function, or *training* or *building* a model. These terms are all synonyms and are \n",
    "used interchangeably in the machine learning literature.\n",
    "- **Examples**: These are what you probably know as *data points* or *observations* or *rows*. \n",
    "- **Features**: These are what you probably know as *independent variables*, *attributes*, *predictors*, \n",
    "or *explanatory variables.*\n",
    "- **Underfitting**: This happens when a model is too simple and does not capture the structure of the data well \n",
    "enough.\n",
    "- **Overfitting**: This happens when a model is too complex or too sensitive to the noise in the data; this can\n",
    "result in poor generalization performance, or applicability of the model to new data. \n",
    "- **Regularization**: This is a general method to avoid overfitting by applying additional constraints to the model. \n",
    "For example, you can limit the number of features present in the final model, or the weight coefficients applied\n",
    "to the (standardized) features are small.\n",
    "- **Supervised learning** involves problems with one target or outcome variable (continuous or discrete) that we want\n",
    "to predict, or classify data into. Classification, prediction, and regression fall into this category. We call the\n",
    "set of explanatory variables $X$ **features**, and the outcome variable of interest $Y$ the **label**.\n",
    "- **Unsupervised learning** involves problems that do not have a specific outcome variable of interest, but rather\n",
    "we are looking to understand \"natural\" patterns or groupings in the data - looking to uncover some structure that \n",
    "we do not know about a priori. Clustering is the most common example of unsupervised learning, another example is \n",
    "principal components analysis (PCA).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We'll be using [`scikit-learn`](http://scikit-learn.org) for the machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_recall_curve,roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score, auc\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Machine Learning Process\n",
    "\n",
    "The Machine Learning Process is as follows:\n",
    "\n",
    "- **Understand the problem and goal.** *This sounds obvious but is often nontrivial.* Problems typically start as vague descriptions of a goal - improving health outcomes, increasing graduation rates, understanding the effect of a variable *X* on an outcome *Y*, etc. It is really important to work with people who understand the domain being studied to dig deeper and define the problem more concretely. What is the analytical formulation of the metric that you are trying to optimize?\n",
    "- **Formulate it as a machine learning problem.** Is it a classification problem or a regression problem? Is the goal to build a model that generates a ranked list prioritized by risk, or is it to detect anomalies as new data come in? Knowing what kinds of tasks machine learning can solve will allow you to map the problem you are working on to one or more machine learning settings and give you access to a suite of methods.\n",
    "- **Data exploration and preparation.** Next, you need to carefully explore the data you have. What additional data do you need or have access to? What variable will you use to match records for integrating different data sources?  What variables exist in the data set? Are they continuous or categorical? What about missing values? Can you use the variables in their original form, or do you need to alter them in some way?\n",
    "- **Feature engineering.** In machine learning language, what you might know as independent variables or predictors or factors or covariates are called \"features.\" Creating good features is probably the most important step in the machine learning process. This involves doing transformations, creating interaction terms, or aggregating over data points or over time and space.\n",
    "- **Method selection.** Having formulated the problem and created your features, you now have a suite of methods to choose from. It would be great if there were a single method that always worked best for a specific type of problem. Typically, in machine learning, you take a variety of methods and try them, empirically validating which one is the best approach to your problem.\n",
    "- **Evaluation.** As you build a large number of possible models, you need a way choose the best among them. We'll cover methodology to validate models on historical data and discuss a variety of evaluation metrics. \n",
    "- **Deployment.** Once you have selected the best model and validated it using historical data as well as a field trial, you are ready to put the model into practice. You still have to keep in mind that new data will be coming in, and the model might change over time.\n",
    "\n",
    "You're probably used to fitting models in social science classes. In those cases, you probably had a hypothesis or theory about the underlying process that gave rise to your data, chose an appropriate model based on prior knowledge and fit it using least squares, and used the resulting parameter or coefficient estimates (or confidence intervals) for inference. This type of modeling is very useful for *interpretation*.\n",
    "\n",
    "In machine learning, our primary concern is *generalization*. This means that:\n",
    "- **We care less about the structure of the model and more about the performance** This means that we'll try out a whole bunch of models at a time and choose the one that works best, rather than determining which model to use ahead of time. We can then choose to select a *suboptimal* model if we care about a specific model type. \n",
    "- **We don't (necessarily) want the model that best fits the data we've *already seen*,** but rather the model that will perform the best on *new data*. This means that we won't gauge our model's performance using the same data that we used to fit the model (e.g., sum of squared errors or $R^2$), and that \"best fit\" or accuracy will most often *not* determine the best model.  \n",
    "- **We can include a lot of variables in to the model.** This may sound like the complete opposite of what you've heard in the past, and it can be hard to swallow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "First, turning something into a real objective function. What do you care about? Do you have data on that thing? What action can you take based on your findings? Do you risk introducing any bias based on the way you model something? \n",
    "\n",
    "### Four Main Types of ML Tasks for Policy Problems\n",
    "\n",
    "- **Description**: [How can we identify and respond to the most urgent online government petitions?](https://dssg.uchicago.edu/project/improving-government-response-to-citizen-requests-online/)\n",
    "- **Prediction**: [Which students will struggle academically by third grade?](https://dssg.uchicago.edu/project/predicting-students-that-will-struggle-academically-by-third-grade/)\n",
    "- **Detection**: [Which police officers are likely to have an adverse interaction with the public?](https://dssg.uchicago.edu/project/expanding-our-early-intervention-system-for-adverse-police-interactions/)\n",
    "- **Behavior Change**: [How can we prevent juveniles from interacting with the criminal justice system?](https://dssg.uchicago.edu/project/preventing-juvenile-interactions-with-the-criminal-justice-system/)\n",
    "  \n",
    "### Our Machine Learning Problems\n",
    "> We will first use an unsupervised method to cluster grnats and then see if we can predict high impact/breakthrough grants based on the information we have in the project data using a supervised model\n",
    "\n",
    "This is an example of a *binary prediction classification problem*. In this case, we are trying to predict a binary outcome: whether a patent is a impactful patent or not.\n",
    "\n",
    "Note that the way the outcome is defined is somewhat arbitrary. You may decide to define the label in a different way, or consider different features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation\n",
    "\n",
    "During the first classes, we have explored the data, linked different data sources, and created new variables. The first steps of creating a machine learning model are exactly that. \n",
    "\n",
    "1. **Creating labels**: Labels are the dependent variables, or *Y* variables, that we are trying to predict. In the machine learning framework, labels are often *binary*: true or false, encoded as 1 or 0. This outcome variable is named `label`.\n",
    "\n",
    "1. **Decide on feature**: Our features are our independent variables or predictors. Good features make machine learning systems effective. The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. Machine Learning Algorithms learn a solution to a problem from sample data. The set of features is the best representation of the sample data to learn a solution to a problem.\n",
    "\n",
    "1. **Feature engineering** is the process of transforming raw data into features that better represent the underlying problem/data/structure  to the predictive models, resulting in improved model accuracy on unseen data.\" ( from [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/) ).  In text, for example, this might involve deriving traits of the text like word counts, verb counts, or topics to feed into a model rather than simply giving it the raw text. Example of feature engineering are: \n",
    "    - **Transformations**, such a log, square, and square root.\n",
    "    - **Dummy (binary) variables**, sometimes known as *indicator variables*, often done by taking categorical variables (such as industry) which do not have a numeric value, and adding them to models as a binary value.\n",
    "    - **Discretization**. Several methods require features to be discrete instead of continuous. This is often done by binning, which you can do by equal width, deciles, Fisher-Jenks, etc. \n",
    "    - **Aggregation.** Aggregate features often constitute the majority of features for a given problem. These use different aggregation functions (*count, min, max, average, standard deviation, etc.*) which summarize several values into one feature, aggregating over varying windows of time and space. For example, we may want to calculate the *number* (and *min, max, mean, variance*, etc.) of crimes within an *m*-mile radius of an address in the past *t* months for varying values of *m* and *t*, and then use all of them as features.\n",
    "\n",
    "1. **Cleaning data**: To run the `scikit-learn` set of models your input dataset must have no missing variables.\n",
    "\n",
    "1. **Imputing values to missing or irrelevant data**: Once the features are created, always check to make sure the values make sense. You might have some missing values, or impossible values for a given variable (negative values, major outliers). If you have missing values you should think hard about what makes the most sense for your problem; you may want to replace with `0`, the median or mean of your data, or some other value.\n",
    "\n",
    "1. **Scaling features**: Certain models will have an issue with features on different scales. For example, an individual's age is typically a number between 0 and 100 while earnings can be number between 0 and 1000000 (or higher). In order to circumvent this problem, we can scale our features to the same range (eg [0,1])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My data directory\n",
    "data_dir = \"/home/jovyan/Yandex.Disk/BigDataPubPol/data\"\n",
    "print( \"The data directory for the class data is \" + data_dir )\n",
    "# Now we are switching into the folder that has the patent data\n",
    "os.chdir(data_dir + \"/projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an empty dataframe that will hold all the patent data we have\n",
    "grants_1012 = pd.DataFrame([])\n",
    "\n",
    "# Now loop through each file in the folder that starts with patent\n",
    "# And append it to the dataframe that we created above\n",
    "\n",
    "for i in range(2010,2013):\n",
    "    g = pd.read_csv(\"FedRePORTER_PRJ_C_FY{}.csv\".format(i), low_memory=False, skipinitialspace=True,\n",
    "                    usecols=['PROJECT_ID','PROJECT_TERMS','PROJECT_TITLE','DEPARTMENT','AGENCY',\n",
    "                             'PROJECT_START_DATE','PROJECT_END_DATE','CONTACT_PI_PROJECT_LEADER', \n",
    "                             'CONGRESSIONAL_DISTRICT','ORGANIZATION_NAME','ORGANIZATION_CITY',\n",
    "                             'ORGANIZATION_STATE','ORGANIZATION_ZIP','ORGANIZATION_COUNTRY',\n",
    "                             'FY','FY_TOTAL_COST'])\n",
    "    grants_1012 = grants_1012.append(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the colums and frequency counts\n",
    "grants_1012.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does our data look like?\n",
    "grants_1012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation - Unsupervised Learning\n",
    "\n",
    "Unsupervised Learning is a class of Machine Learning techniques to find the patterns in data. The data given to unsupervised algorithm are not labelled, which means only the input variables(X) are given with no corresponding output variables. In unsupervised learning, the algorithms are left to themselves to discover interesting structures in the data. So a problem formulation in our case would be identifying similar research projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "\n",
    "Let's run a K-Means cluster analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are our features again?\n",
    "print(grants_1012.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means does not support missing vlaues. You need clean data (delete or impute)\n",
    "print(grants_1012.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to carefully think how to impute missing values or if you want to drop them\n",
    "# Beacause it takes too long to clean data we are dropping them for now\n",
    "grants_1012 = grants_1012.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important to understand the features (categorical vs. numerical)\n",
    "grants_1012.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two lines of code. In the first line, we create a KMeans object and pass it 3 as value for n_clusters parameter (you can set this parameter as you wish). Next, we call the fit method on kmeans and pass the data we want to cluster, which in this case is the grants data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a cluster analysis (k-means we need numerical data).But we can create some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For terms, title we can count the words\n",
    "# For terms, title we can count the words\n",
    "grants_1012['tit_len'] = grants_1012['PROJECT_TITLE'].str.split().str.len()\n",
    "grants_1012['terms_num'] = grants_1012['PROJECT_TERMS'].str.split(';').str.len()\n",
    "\n",
    "# We just need the yearfo start and end\n",
    "grants_1012['project_start'] = grants_1012['PROJECT_START_DATE'].str.extract('(\\d\\d\\d\\d)', expand=True).astype(int)\n",
    "grants_1012['project_end'] = grants_1012['PROJECT_END_DATE'].str.extract('(\\d\\d\\d\\d)', expand=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the numerical variables\n",
    "df_cluster = grants_1012[['project_start','project_end','FY_TOTAL_COST','FY','tit_len','terms_num']]\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cluster algorithm\n",
    "kmeans = KMeans(n_clusters=3) \n",
    "kmeans.fit(df_cluster)\n",
    "labels = kmeans.predict(df_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check centroid values the algorithm generated for the final clusters\n",
    "print(kmeans.cluster_centers_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add back to original data\n",
    "df_cluster['clusters'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets analyze the clusters\n",
    "print(df_cluster.groupby(['clusters']).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us what the differences between the clusters are. It shows mean values of the attribute per each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of Number of Terms and Total Cost\n",
    "sns.lmplot('terms_num', 'FY_TOTAL_COST', \n",
    "           data=df_cluster, \n",
    "           fit_reg=False, \n",
    "           hue=\"clusters\",  \n",
    "           scatter_kws={\"marker\": \"D\", \n",
    "                        \"s\": 100})\n",
    "plt.title('Terms vs Funding Amount')\n",
    "plt.xlabel('Number of Terms')\n",
    "plt.ylabel('Funding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Checks\n",
    "\n",
    "You can check the quality of your cluster by invetigating how well it fits to the data. A good cluster has all the data close together and not spread out. We can measure the distance from the centroid for each cluster. Lower is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each k value, we will initialise k-means and use the inertia attribute to identify the sum of squared distances of samples to the nearest cluster centre. As k increases, the sum of squared distance tends to zero. Imagine we set k to its maximum value n (where n is number of samples) each sample will form its own cluster meaning sum of squared distances equals zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster.drop(['clusters'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(df_cluster)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result: Elbow plot\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation - Supervised Learning\n",
    "Now lets use a supervised model to seeif we can predict grants with high impact.  We first need to define our outcome. What is a grant. with high impact? There are many ways to measure this. For now we will go with the funding amount and say that whatever is in the tp 75-perecentile has high impact. Then we have to build the features and build our test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_1012.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Model\n",
    "\n",
    "We need to devide our dataset into **features** (predictors, or independent variables, or $X$ variables) and **labels** (dependent variables, or $Y$ variables).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Labels\n",
    "\n",
    "Labels are the dependent variables, or *Y* variables, that we are trying to predict. In the machine learning framework, your labels are usually *binary*: true or false, encoded as 1 or 0. In this case, our label is whether a grant has published a paper. So our label *Y* is 0 if there are no publications associated with a grant, and is 1 if a grant was able to publish a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_1012['FY_TOTAL_COST'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grants_1012['Y'] = np.where(grants_1012['FY_TOTAL_COST'] >= 4.144582e+05 , 1, 0)\n",
    "grants_1012['Y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Generation\n",
    "\n",
    "Our features are our independent variables or predictors. Good features make machine learning systems effective. \n",
    "The better the features the easier it is the capture the structure of the data. You generate features using domain knowledge. In general, it is better to have more complex features and a simpler model rather than vice versa. Keeping the model simple makes it faster to train and easier to understand rather then extensively searching for the \"right\" model and \"right\" set of parameters. \n",
    "\n",
    "Our features are the following\n",
    "\n",
    "- `state`: The state in which the applicant organization is located (we need to make dummies)\n",
    "\n",
    "- `project_start`: Start of project, dummies\n",
    "\n",
    "- `project_end`: End of project, dummies\n",
    "\n",
    "- `terms_num`: Number of words in project terms\n",
    "\n",
    "- `tit_len`: Length of Project Title\n",
    "\n",
    "- `fy`: Year, we need to make a dummy here too\n",
    "\n",
    "- `agency`: The funding agency, we need to make a dummy here too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = grants_1012[['project_start','project_end','Y','FY','tit_len','terms_num','ORGANIZATION_STATE','AGENCY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to creat dummy variables for state and organization name\n",
    "df = pd.get_dummies(df, columns=['ORGANIZATION_STATE', 'FY', 'AGENCY', 'project_start', 'project_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n",
    "\n",
    "It's not enough to just build the model; we're going to need a way to know whether or not it's working. Convincing others of the quality of results is often the *most challenging* part of an analysis.  Making repeatable, well-documented work with clear success metrics makes all the difference.\n",
    "\n",
    "To convince ourselves - and others - that our modeling results will generalize, we need to hold\n",
    "some data back (not using it to train the model), then apply our model to that hold-out set and \"blindly\" predict, comparing the model's predictions to what we actually observed. This is called **cross-validation**, and it's the best way we have to estimate how a model will perform on *entirely* novel data. We call the data used to build the model the **training set**, and the rest the **test set**.\n",
    "\n",
    "For our project we will start dividing the sample to have a training and test set. There are mulriple ways to define training and test set. You can try different definitions on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily split data by using a pre-defined function from the sklearn package\n",
    "df_train, df_test = train_test_split(df, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation \n",
    "\n",
    "It is important to to do a quick check of our matrix to see if we have any outlier values. We might have some problems with the publication count. However in our example it is fine because we are only looking at binary outcomes. Also you want to investigate missing values. You can't have missing values sickit models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Balancing\n",
    "\n",
    "Let's check how much data we still have and how many end in publications in our training dataset. We don't necessarily need to have a perfect 50-50 balance, but it's good to know what the \"baseline\" is in our dataset, to be able to intelligently evaluate our performance. If you look at our Y you can see that not that many grant result in publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_train.shape[0]))\n",
    "df_train['Y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows: {}'.format(df_test.shape[0]))\n",
    "df_test['Y'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate our data\n",
    "\n",
    "We can use visualizations to find trends and patterns in our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"Y\", y=\"terms_num\", data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x=\"Y\", y=\"tit_len\", data=df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = [col for col in df_train if col.startswith('ORGANIZATION')]\n",
    "project = [col for col in df_train if col.startswith('project')]\n",
    "\n",
    "sel_features = ['FY_2010','FY_2011','FY_2012', 'terms_num','tit_len', 'AGENCY_AHRQ','AGENCY_ALLCDC','AGENCY_ARS','AGENCY_CDMRP','AGENCY_CNRM','AGENCY_FDA','AGENCY_FS',                \n",
    "'AGENCY_IES','AGENCY_NASA','AGENCY_NIDILRR','AGENCY_NIFA','AGENCY_NIH','AGENCY_NSF'] + org + project\n",
    "sel_label = 'Y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use conventions typically used in python scikitlearn\n",
    "\n",
    "X_train = df_train[sel_features].values\n",
    "y_train = df_train[sel_label].values\n",
    "X_test = df_test[sel_features].values\n",
    "y_test = df_test[sel_label].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation \n",
    "\n",
    "In this phase, you take the predictors from your test set and apply your model to them, then assess the quality of the model by comparing the *predicted values* to the *actual values* for each record in your testing data set. \n",
    "\n",
    "- **Performance Estimation**: How well will our model do once it is deployed and applied to new data?\n",
    "\n",
    "Now let's use the model we just fit to make predictions on our test dataset, and see what our accuracy score is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's [`scikit-learn`](http://scikit-learn.org/stable/) is a commonly used, well documented Python library for machine learning. This library can help you split your data into training and test sets, fit models and use them to predict results on new data, and evaluate your results.\n",
    "\n",
    "We will start with the simplest [`LogisticRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model and see how well that does.\n",
    "\n",
    "You can use any number of metrics to judge your models, but we'll use [`accuracy_score()`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) (ratio of correct predictions to total number of predictions) as our measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fit a model\n",
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression(penalty='l1', C=1e5)\n",
    "model.fit( X_train, y_train )\n",
    "print(model)\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print the model results, we see different parameters we can adjust as we refine the model based on running it against test data (values such as `intercept_scaling`, `max_iters`, `penalty`, and `solver`).  \n",
    "\n",
    "Penalty L1: Lasso shrinks the less important feature’s coefficient to zero thus, removing some feature altogether. So, this works well for feature selection in case we have a huge number of features.\n",
    "\n",
    "C: Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization.\n",
    "\n",
    "To adjust these parameters, one would alter the call that creates the `LogisticRegression()` model instance, passing it one or more of these parameters with a value other than the default.  So, to re-fit the model with `max_iter` of 1000, `intercept_scaling` of 2, and `solver` of \"lbfgs\" (pulled from thin air as an example), you'd create your model as follows:\n",
    "\n",
    "    model = LogisticRegression( max_iter = 1000, intercept_scaling = 2, solver = \"lbfgs\" )\n",
    "\n",
    "The basic way to choose values for, or \"tune,\" these parameters is the same as the way you choose a model: fit the model to your training data with a variety of parameters, and see which perform the best on the test set. An obvious drawback is that you can also *overfit* to your test set; in this case, you can alter your method of cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "\n",
    "Machine learning models usually do not produce a prediction (0 or 1) directly. Rather, models produce a score between 0 and 1 (that can sometimes be interpreted as a probability), which lets you more finely rank all of the examples from *most likely* to *least likely* to have label 1 (positive). This score is then turned into a 0 or 1 based on a user-specified threshold. For example, you might label all examples that have a score greater than 0.5 (1/2) as positive (1), but there's no reason that has to be the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  from our \"predictors\" using the model.\n",
    "y_scores = model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the distribution of scores and see if it makes sense to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y_scores, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['y_score'] = y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['y_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools like `sklearn` often have a default threshold of 0.5, but a good threshold is selected based on the data, model and the specific problem you are solving. As a trial run, let's set a threshold of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_threshold = lambda x,y: 0 if x < y else 1 \n",
    "predicted = np.array( [calc_threshold(score,0.50) for score in y_scores] )\n",
    "expected = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(expected, kde=False, rug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Once we have tuned our scores to 0 or 1 for classification, we create a *confusion matrix*, which  has four cells: true negatives, true positives, false negatives, and false positives. Each data point belongs in one of these cells, because it has both a ground truth and a predicted label. If an example was predicted to be negative and is negative, it's a true negative. If an example was predicted to be positive and is positive, it's a true positive. If an example was predicted to be negative and is positive, it's a false negative. If an example was predicted to be positive and is negative, it's a false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(expected,predicted)\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of true negatives is `conf_matrix[0,0]`, false negatives `conf_matrix[1,0]`, true positives `conf_matrix[1,1]`, and false_positives `conf_matrix[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is the ratio of the correct predictions (both positive and negative) to all predictions. \n",
    "$$ Accuracy = \\frac{TP+TN}{TP+TN+FP+FN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an accuracy score by comparing expected to predicted.\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(expected, predicted)\n",
    "print( \"Accuracy = \" + str( accuracy ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two additional metrics that are often used are **precision** and **recall**. \n",
    "\n",
    "Precision measures the accuracy of the classifier when it predicts an example to be positive. It is the ratio of correctly predicted positive examples to examples predicted to be positive. \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "Recall measures the accuracy of the classifier to find positive examples in the data. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "By selecting different thresholds we can vary and tune the precision and recall of a given classifier. A conservative classifier (threshold 0.99) will classify a case as 1 only when it is *very sure*, leading to high precision. On the other end of the spectrum, a low threshold (e.g. 0.01) will lead to higher recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(expected, predicted)\n",
    "recall = recall_score(expected, predicted)\n",
    "print( \"Precision = \" + str( precision ) )\n",
    "print( \"Recall= \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we care about our whole precision-recall space, we can optimize for a metric known as the **area under the curve (AUC-PR)**, which is the area under the precision-recall curve. The maximum AUC-PR is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score)\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(expected, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall at k%\n",
    "\n",
    "If we only care about a specific part of the precision-recall curve we can focus on more fine-grained metrics. For instance, say there is a year where an agency has funding constraints. They have less money to allocate in this given year. In that case, we would want to prioritize the 1% of applicants who are *most likely* to publish, and it doesn't matter too much how accurate we are on the ones who aren't very likely to publish. We can then focus on optimizing our **precision at 1%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_n(y_true, y_prob, model_name):\n",
    "    \"\"\"\n",
    "    y_true: ls \n",
    "        ls of ground truth labels\n",
    "    y_prob: ls\n",
    "        ls of predic proba from model\n",
    "    model_name: str\n",
    "        str of model name (e.g, LR_123)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax1.set_ylim(0,1.05)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax2.set_ylim(0,1.05)\n",
    "    \n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(y_true, y_scores,k):\n",
    "    \n",
    "    threshold = np.sort(y_scores)[::-1][int(k*len(y_scores))]\n",
    "    y_pred = np.asarray([1 if i >= threshold else 0 for i in y_scores ])\n",
    "    return precision_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_n(expected,y_scores, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_at_1 = precision_at_k(expected,y_scores, 0.01)\n",
    "print('Precision at 1%: {:.2f}'.format(p_at_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess Model Against Baselines\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "It is important to check our model against a reasonable **baseline** to know how well our model is doing. Without any context, 78% accuracy can sound really great... but it's not so great when you remember that you could do almost that well by declaring everyone will not need benefits in the next year, which would be stupid (not to mention useless) model. \n",
    "\n",
    "A good place to start is checking against a *random* baseline, assigning every example a label (positive or negative) completely at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_score = [random.uniform(0,1) for i in enumerate(y_test)] \n",
    "random_predicted = np.array( [calc_threshold(score,0.5) for score in random_score] )\n",
    "random_p_at_5 = precision_at_k(expected,random_predicted, 0.01)\n",
    "print('Precision at 1% (random): {:.2f}'.format(random_p_at_5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Linear regression and logistic regression models fail in situations where the relationship between features and outcome is nonlinear or where features interact with each other. Tree based models split the data multiple times according to certain cutoff values in the features. Through splitting, different subsets of the dataset are created, with each instance belonging to one subset. The final subsets are called terminal or leaf nodes and the intermediate subsets are called internal nodes or split nodes. To predict the outcome in each leaf node, the average outcome of the training data in this node is used. Trees can be used for classification and regression. Here is more info on how to interpret the tree: http://engineering.pivotal.io/post/interpreting-decision-trees-and-random-forests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
    "                               max_depth=3, min_samples_leaf=5)\n",
    "clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import graphviz \n",
    "dot_data = tree.export_graphviz(clf_gini, out_file=None) \n",
    "graph = graphviz.Source(dot_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this package is not installed in our environment yet. You can create your own virtual environment and install it there typing conda install python-graphviz in the terminal. It look should something like this: https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning: Document Classification\n",
    "\n",
    "Previously, we used topic modeling to infer relationships of abstracts of funded research projects. That is an example of unsupervised learning: we were looking to uncover structure in the form of topics, or groups of agencies, but we did not necessarily know the ground truth of how many groups we should find or which agencies belonged in which group.  \n",
    "\n",
    "Now we turn our attention to supervised learning. In supervised learning, we have a *known* outcome or label (*Y*) that we want to produce given some data (*X*), and in general, we want to be able to produce this *Y* when we *don't* know it, or when we *only* have *X*. \n",
    "\n",
    "In order to produce labels we need to first have examples our algorithm can learn from, a \"training set.\" In the context of text analysis, developing a training set can be very expensive, as it can require a large amount of human labor or linguistic expertise. **Document classification** is an example of supervised learning in which want to characterize our documents based on their contents (*X*). A common example of document classification is spam e-mail detection. Another example of supervised learning in text analysis is *sentiment analysis*, where *X* is our documents and *Y* is the state of the author. This \"state\" is dependent on the question you're trying to answer, and can range from the author being happy or unhappy with a product to the author being politically conservative or liberal. Another example is *part-of-speech tagging* where *X* are individual words and *Y* is the part-of-speech. \n",
    "\n",
    "In this section, we'll train a classifier to classify abstract of grant proposals. Let's see if we can label a new abstract as belonging to a high paying research grant or low paying grant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning - Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe\n",
    "abstr = (pd.read_csv(str(data_dir) + \"abstracts/FedRePORTER_PRJABS_C_FY2017.csv\",\n",
    "                  skipinitialspace=True, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with project data\n",
    "proj = (pd.read_csv(str(data_dir) + \"projects/FedRePORTER_PRJ_C_FY2017.csv\",\n",
    "                  skipinitialspace=True, encoding='utf-8'))\n",
    "abstracts = pd.merge(abstr, proj[['FY_TOTAL_COST', 'PROJECT_ID']], on='PROJECT_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mask column we can use to flag rows with facility type in our types of interest.\n",
    "abstracts['value'] = np.where((abstracts['FY_TOTAL_COST'] >= 100000) , 'High', 'Low')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop missing values\n",
    "abstracts = abstracts.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets.\n",
    "df_train, df_test = train_test_split(abstracts, test_size=0.20, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at our training set.\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we only have the types we expect.\n",
    "df_train['value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the counts for each value.\n",
    "Counter(df_train['value'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at our testing set.\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we only have the facility types we expect.\n",
    "df_test['value'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the counts for each value.\n",
    "Counter(df_test['value'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Document Classification\n",
    "\n",
    "In order to feed out data into a classifier, we need to pull out the labels (*Y*) and a clean corpus of documents (*X*) for our training and testing sets. You want to clean the text data before doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data - get labels we'll train on.\n",
    "train_labels = df_train.value.values\n",
    "\n",
    "# prepare training data - clean text.\n",
    "train_corpus = np.array(df_train.ABSTRACT.values)\n",
    "\n",
    "# prepare testing data - get labels we'll train on.\n",
    "test_labels = df_test.value.values\n",
    "\n",
    "# prepare testing data - clean text.\n",
    "test_corpus = np.array(df_test.ABSTRACT.values)\n",
    "\n",
    "# make list of all labels across train and test (should just be 'income' and 'health')\n",
    "labels = np.append(train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we had done in the unsupervised learning context, we have to transform our data. This time we have to transform our testing and training set into two different bags of words. The classifier will learn from the training set, and we will evaluate the classifier's performance on the testing set.\n",
    "\n",
    "First, we create a CountVectorizer that we'll use to convert our text documents to matrices of features based on words contained within our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for vectorizer \n",
    "ANALYZER = \"word\" #unit of features are single words rather then phrases of words \n",
    "STRIP_ACCENTS = 'unicode'\n",
    "TOKENIZER = None\n",
    "NGRAM_RANGE = (0,2) #Range for pharases of words\n",
    "MIN_DF = 0.01 # Exclude words that have a frequency less than the threshold\n",
    "MAX_DF = 0.8  # Exclude words that have a frequency greater then the threshold \n",
    "\n",
    "vectorizer = CountVectorizer( analyzer = ANALYZER,\n",
    "                              tokenizer = None, # alternatively tokenize_and_stem but it will be slower \n",
    "                              ngram_range = NGRAM_RANGE,\n",
    "                              stop_words = stopwords.words( 'english' ),\n",
    "                              strip_accents = STRIP_ACCENTS,\n",
    "                              min_df = MIN_DF,\n",
    "                              max_df = MAX_DF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a TF-IDF transformer, and create our bags of words, then weight them using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM = None          # turn on normalization flag\n",
    "SMOOTH_IDF = True    # prevents division by zero errors\n",
    "SUBLINEAR_IDF = True # replace TF with 1 + log(TF)\n",
    "USE_IDF = True       # flag to control whether to use TFIDF\n",
    "\n",
    "transformer = TfidfTransformer( norm = NORM,\n",
    "                                smooth_idf = SMOOTH_IDF,\n",
    "                                sublinear_tf = True )\n",
    "\n",
    "# timing code - start!\n",
    "start_time = time.time()\n",
    "\n",
    "# get the bag-of-words for train and test from the vectorizer and\n",
    "# then use TFIDF to limit the tokens found throughout the text \n",
    "train_bag_of_words = vectorizer.fit_transform( train_corpus ) #using all the data on for generating features!! Bad!\n",
    "test_bag_of_words = vectorizer.transform( test_corpus )\n",
    "\n",
    "# if we use IDF, compute it here.\n",
    "if USE_IDF:\n",
    "    train_tfidf = transformer.fit_transform(train_bag_of_words)\n",
    "    test_tfidf = transformer.transform(test_bag_of_words)\n",
    "\n",
    "# Get list of the feature names, for passing to our model.\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "# timing code - done!\n",
    "print('Time Elapsed: {0:.2f}s'.format( time.time() - start_time ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot pass the labels directly to the classifier. Instead, we to encode them as 0s and 1s using the `labelencoder` part of `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel our labels as a 0 or 1\n",
    "le = preprocessing.LabelEncoder() \n",
    "le.fit(labels)\n",
    "labels_binary = le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(labels,labels_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create arrays of indices so we can access the training and testing sets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = df_train.shape[ 0 ]\n",
    "train_set_idx = np.arange( 0, train_size )\n",
    "test_set_idx = np.arange( train_size, len( labels ) )\n",
    "train_labels_binary = labels_binary[ train_set_idx ]\n",
    "test_labels_binary = labels_binary[ test_set_idx ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - Train Document Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier we are using in the example is LogisticRegression. As we saw in the Machine Learning tutorial, first we decide on a classifier, then we fit the classifier to the data to create a model. We can then test our model on the test set by passing the features (*X*) from our test set to get predicted labels. The model will output the probability of each abstract being classified as low or high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our LogisticRegression classifier.\n",
    "clf = LogisticRegression(penalty='l1')\n",
    "\n",
    "# train the classifer to create our model.\n",
    "mdl = clf.fit( train_tfidf, labels_binary[ train_set_idx ] )\n",
    "\n",
    "# create scores for each of the documents predicting whether each refers to \n",
    "#     an income or health agency\n",
    "y_score = mdl.predict_proba( test_tfidf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Precision and Recall\n",
    "\n",
    "Now that we have calculated a score for each of our facility types of interest, we look at how well our model performed by outputting precision and recall curves at different cutoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_n( labels_binary[ test_set_idx ], y_score[:,1], 'LR' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can try to maximize the entire precision-recall space. In this case we need a different metric - \"Area Under Curve\" (AUC). The AUC shows how accurate our scores are under different cutoff thresholds. The model will output a score between 0 and 1. We specify a  range of cutoff values and label all of the examples as 0 or 1 based on whether they are above or below each cutoff value. The closer our scores are to the true values, the more resilient they are to different cutoffs. For instance, if our scores were perfect, our AUC would be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(y_true,y_score):\n",
    "    \"\"\"\n",
    "    Plot a precision recall curve\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: ls\n",
    "        ground truth labels\n",
    "    y_score: ls\n",
    "        score output from model\n",
    "    \"\"\"\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true,y_score[:,1])\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    auc_val = auc(recall_curve,precision_curve)\n",
    "    print('AUC-PR: {0:1f}'.format(auc_val))\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall(labels_binary[test_set_idx],y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation - Feature Importances\n",
    "\n",
    "Next, we look at the importance of different features (words) in our model.\n",
    "\n",
    "The function that will calculate these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_importances( coef, features, labels, num_features = 10 ):\n",
    "\n",
    "    \"\"\"\n",
    "    output feature importances\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    coef: numpy\n",
    "        feature importances\n",
    "    features: ls \n",
    "        feature names\n",
    "    labels: ls\n",
    "        labels for the classifier\n",
    "    num_features: int\n",
    "        number of features to output (default 10)\n",
    "    \n",
    "    Example\n",
    "    --------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    coef = mdl.coef_.ravel()\n",
    "\n",
    "    dict_feature_importances = dict( zip(features, coef) )\n",
    "    orddict_feature_importances = OrderedDict( \n",
    "                                    sorted(dict_feature_importances.items(), key=lambda x: x[1]) )\n",
    "\n",
    "    ls_sorted_features  = list(orddict_feature_importances.keys())\n",
    "\n",
    "    label0_features = ls_sorted_features[:num_features] \n",
    "    label1_features = ls_sorted_features[-num_features:] \n",
    "\n",
    "    print(labels[0],label0_features)\n",
    "    print(labels[1], label1_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_feature_importances(mdl.coef_.ravel(), features, ['high','low'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importances give us the words which are the most relevant for distinguishing the type of grant (between high or low funded grant). Some of these make sense, but some don't make as much sense, or seem to be artifacts that we should remove. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "When working on machine learning projects, it is a good idea to structure your code as a modular **pipeline**, which contains all of the steps of your analysis, from the original data source to the results that you report, along with documentation. This has many advantages:\n",
    "- **Reproducibility**. It's important that your work be reproducible. This means that someone else should be able\n",
    "to see what you did, follow the exact same process, and come up with the exact same results. It also means that\n",
    "someone else can follow the steps you took and see what decisions you made, whether that person is a collaborator, \n",
    "a reviewer for a journal, or the agency you are working with. \n",
    "- **Ease of model evaluation and comparison**.\n",
    "- **Ability to make changes.** If you receive new data and want to go through the process again, or if there are \n",
    "updates to the data you used, you can easily substitute new data and reproduce the process without starting from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "*[Go back to Table of Contents](#Table-of-Contents)*\n",
    "\n",
    "- Hastie et al.'s [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) is a classic and is available online for free.\n",
    "- James et al.'s [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), also available online, includes less mathematics and is more approachable.\n",
    "- Wu et al.'s [Top 10 Algorithms in Data Mining](http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
